{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "train_data = pd.read_excel(\"data/cases_2021_train_processed_2.xlsx\")\n",
    "test_data = pd.read_excel(\"data/cases_2021_test_processed_unlabelled_2.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_data.copy()\n",
    "train = train[['age', 'country', 'chronic_disease_binary', 'Case_Fatality_Ratio','outcome_group']]\n",
    "test = test_data.copy()\n",
    "test = test[['age', 'country', 'chronic_disease_binary', 'Case_Fatality_Ratio']]\n",
    "train_data, test_data = train,test "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Feature Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_data.copy()\n",
    "train['country'] = pd.factorize(train['country'])[0]\n",
    "train['chronic_disease_binary'] = pd.factorize(train['chronic_disease_binary'])[0]\n",
    "new_label = {\"outcome_group\": {\"deceased\": 0, \"hospitalized\": 1, \"nonhospitalized\": 2}}\n",
    "train.replace(new_label, inplace = True)\n",
    "test = test_data.copy()\n",
    "test['country'] = pd.factorize(test['country'])[0]\n",
    "test['chronic_disease_binary'] = pd.factorize(test['chronic_disease_binary'])[0]\n",
    "train_data, test_data = train,test "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Balancing Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_train_dataset_pie_chart(train_dataset: pd.DataFrame, title: str):\n",
    "    plt.figure()\n",
    "    data = train_dataset.groupby(\"outcome_group\").size()\n",
    "    print(\"\\n\" + title + \",\")\n",
    "    print(data)\n",
    "    data = [int(data[0]), int(data[1]), int(data[2])]\n",
    "    labels = [\"deceased\", \"hospitalized\", \"nonhospitalized\"]\n",
    "    colours = sns.color_palette('pastel')[0:4]\n",
    "    plt.pie(x=data, labels=labels, colors=colours, autopct='%.0f%%')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "show_train_dataset_pie_chart(train_data, \"Before Balancing\")\n",
    "\n",
    "deceased = train_data[train_data[\"outcome_group\"] == 0]\n",
    "new_deceased = deceased.sample(frac=10, replace=True, random_state=1)\n",
    "new_deceased.reset_index(inplace=True, drop=True)\n",
    "\n",
    "hospitalized = train_data[train_data[\"outcome_group\"] == 1]\n",
    "hospitalized_sample = np.random.choice(hospitalized.index, 3000, replace=True)\n",
    "new_hospitalized = hospitalized.drop(hospitalized_sample)\n",
    "new_hospitalized.reset_index(inplace=True, drop=True)\n",
    "\n",
    "nonhospitalized = train_data[train_data[\"outcome_group\"] == 2]\n",
    "new_nonhospitalized = nonhospitalized.sample(frac=3.3, replace=True, random_state=1)\n",
    "new_nonhospitalized.reset_index(inplace=True, drop=True)\n",
    "\n",
    "new_train = pd.concat([new_deceased, new_hospitalized, new_nonhospitalized])\n",
    "new_train.sort_index(axis = 0, inplace=True)\n",
    "new_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "show_train_dataset_pie_chart(new_train, \"After Balancing\")\n",
    "\n",
    "train_data = new_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Building Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validation Split,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data = train_test_split(train_data, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV 1/5; 1/12] START learning_rate=0.2, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 1/5; 1/12] END learning_rate=0.2, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.811 total time=   0.6s\n",
      "[CV 2/5; 1/12] START learning_rate=0.2, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 2/5; 1/12] END learning_rate=0.2, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.802 total time=   0.6s\n",
      "[CV 3/5; 1/12] START learning_rate=0.2, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 3/5; 1/12] END learning_rate=0.2, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.818 total time=   0.6s\n",
      "[CV 4/5; 1/12] START learning_rate=0.2, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 4/5; 1/12] END learning_rate=0.2, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.817 total time=   0.6s\n",
      "[CV 5/5; 1/12] START learning_rate=0.2, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 5/5; 1/12] END learning_rate=0.2, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.816 total time=   0.6s\n",
      "[CV 1/5; 2/12] START learning_rate=0.2, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 1/5; 2/12] END learning_rate=0.2, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.812 total time=   1.0s\n",
      "[CV 2/5; 2/12] START learning_rate=0.2, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 2/5; 2/12] END learning_rate=0.2, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.804 total time=   1.0s\n",
      "[CV 3/5; 2/12] START learning_rate=0.2, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 3/5; 2/12] END learning_rate=0.2, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.818 total time=   1.0s\n",
      "[CV 4/5; 2/12] START learning_rate=0.2, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 4/5; 2/12] END learning_rate=0.2, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.817 total time=   1.1s\n",
      "[CV 5/5; 2/12] START learning_rate=0.2, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 5/5; 2/12] END learning_rate=0.2, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.818 total time=   1.3s\n",
      "[CV 1/5; 3/12] START learning_rate=0.2, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 1/5; 3/12] END learning_rate=0.2, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.812 total time=   0.9s\n",
      "[CV 2/5; 3/12] START learning_rate=0.2, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 2/5; 3/12] END learning_rate=0.2, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.806 total time=   0.7s\n",
      "[CV 3/5; 3/12] START learning_rate=0.2, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 3/5; 3/12] END learning_rate=0.2, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.818 total time=   0.8s\n",
      "[CV 4/5; 3/12] START learning_rate=0.2, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 4/5; 3/12] END learning_rate=0.2, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.817 total time=   0.7s\n",
      "[CV 5/5; 3/12] START learning_rate=0.2, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 5/5; 3/12] END learning_rate=0.2, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.818 total time=   0.7s\n",
      "[CV 1/5; 4/12] START learning_rate=0.2, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 1/5; 4/12] END learning_rate=0.2, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.812 total time=   1.3s\n",
      "[CV 2/5; 4/12] START learning_rate=0.2, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 2/5; 4/12] END learning_rate=0.2, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.807 total time=   1.2s\n",
      "[CV 3/5; 4/12] START learning_rate=0.2, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 3/5; 4/12] END learning_rate=0.2, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.819 total time=   1.4s\n",
      "[CV 4/5; 4/12] START learning_rate=0.2, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 4/5; 4/12] END learning_rate=0.2, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.817 total time=   1.4s\n",
      "[CV 5/5; 4/12] START learning_rate=0.2, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 5/5; 4/12] END learning_rate=0.2, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.817 total time=   1.3s\n",
      "[CV 1/5; 5/12] START learning_rate=0.2, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 1/5; 5/12] END learning_rate=0.2, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.813 total time=   0.9s\n",
      "[CV 2/5; 5/12] START learning_rate=0.2, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 2/5; 5/12] END learning_rate=0.2, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.807 total time=   1.0s\n",
      "[CV 3/5; 5/12] START learning_rate=0.2, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 3/5; 5/12] END learning_rate=0.2, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.820 total time=   0.9s\n",
      "[CV 4/5; 5/12] START learning_rate=0.2, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 4/5; 5/12] END learning_rate=0.2, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.817 total time=   0.9s\n",
      "[CV 5/5; 5/12] START learning_rate=0.2, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 5/5; 5/12] END learning_rate=0.2, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.818 total time=   0.9s\n",
      "[CV 1/5; 6/12] START learning_rate=0.2, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 1/5; 6/12] END learning_rate=0.2, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.813 total time=   1.6s\n",
      "[CV 2/5; 6/12] START learning_rate=0.2, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 2/5; 6/12] END learning_rate=0.2, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.807 total time=   1.6s\n",
      "[CV 3/5; 6/12] START learning_rate=0.2, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 3/5; 6/12] END learning_rate=0.2, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.819 total time=   1.6s\n",
      "[CV 4/5; 6/12] START learning_rate=0.2, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 4/5; 6/12] END learning_rate=0.2, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.818 total time=   1.6s\n",
      "[CV 5/5; 6/12] START learning_rate=0.2, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 5/5; 6/12] END learning_rate=0.2, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.817 total time=   1.6s\n",
      "[CV 1/5; 7/12] START learning_rate=0.3, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 1/5; 7/12] END learning_rate=0.3, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.811 total time=   0.5s\n",
      "[CV 2/5; 7/12] START learning_rate=0.3, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 2/5; 7/12] END learning_rate=0.3, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.805 total time=   0.6s\n",
      "[CV 3/5; 7/12] START learning_rate=0.3, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 3/5; 7/12] END learning_rate=0.3, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.818 total time=   0.5s\n",
      "[CV 4/5; 7/12] START learning_rate=0.3, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 4/5; 7/12] END learning_rate=0.3, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.818 total time=   0.6s\n",
      "[CV 5/5; 7/12] START learning_rate=0.3, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 5/5; 7/12] END learning_rate=0.3, max_depth=6, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.818 total time=   0.5s\n",
      "[CV 1/5; 8/12] START learning_rate=0.3, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 1/5; 8/12] END learning_rate=0.3, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.812 total time=   1.0s\n",
      "[CV 2/5; 8/12] START learning_rate=0.3, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 2/5; 8/12] END learning_rate=0.3, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.807 total time=   1.0s\n",
      "[CV 3/5; 8/12] START learning_rate=0.3, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 3/5; 8/12] END learning_rate=0.3, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.818 total time=   1.0s\n",
      "[CV 4/5; 8/12] START learning_rate=0.3, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 4/5; 8/12] END learning_rate=0.3, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.817 total time=   1.0s\n",
      "[CV 5/5; 8/12] START learning_rate=0.3, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 5/5; 8/12] END learning_rate=0.3, max_depth=6, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.818 total time=   1.0s\n",
      "[CV 1/5; 9/12] START learning_rate=0.3, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 1/5; 9/12] END learning_rate=0.3, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.812 total time=   0.8s\n",
      "[CV 2/5; 9/12] START learning_rate=0.3, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 2/5; 9/12] END learning_rate=0.3, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.807 total time=   0.8s\n",
      "[CV 3/5; 9/12] START learning_rate=0.3, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 3/5; 9/12] END learning_rate=0.3, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.818 total time=   0.8s\n",
      "[CV 4/5; 9/12] START learning_rate=0.3, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 4/5; 9/12] END learning_rate=0.3, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.817 total time=   0.8s\n",
      "[CV 5/5; 9/12] START learning_rate=0.3, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 5/5; 9/12] END learning_rate=0.3, max_depth=8, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.818 total time=   0.8s\n",
      "[CV 1/5; 10/12] START learning_rate=0.3, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 1/5; 10/12] END learning_rate=0.3, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.814 total time=   1.3s\n",
      "[CV 2/5; 10/12] START learning_rate=0.3, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 2/5; 10/12] END learning_rate=0.3, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.807 total time=   1.3s\n",
      "[CV 3/5; 10/12] START learning_rate=0.3, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 3/5; 10/12] END learning_rate=0.3, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.819 total time=   1.3s\n",
      "[CV 4/5; 10/12] START learning_rate=0.3, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 4/5; 10/12] END learning_rate=0.3, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.817 total time=   1.3s\n",
      "[CV 5/5; 10/12] START learning_rate=0.3, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 5/5; 10/12] END learning_rate=0.3, max_depth=8, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.818 total time=   1.3s\n",
      "[CV 1/5; 11/12] START learning_rate=0.3, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 1/5; 11/12] END learning_rate=0.3, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.813 total time=   1.0s\n",
      "[CV 2/5; 11/12] START learning_rate=0.3, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 2/5; 11/12] END learning_rate=0.3, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.807 total time=   0.9s\n",
      "[CV 3/5; 11/12] START learning_rate=0.3, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 3/5; 11/12] END learning_rate=0.3, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.819 total time=   0.9s\n",
      "[CV 4/5; 11/12] START learning_rate=0.3, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 4/5; 11/12] END learning_rate=0.3, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.817 total time=   0.9s\n",
      "[CV 5/5; 11/12] START learning_rate=0.3, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax\n",
      "[CV 5/5; 11/12] END learning_rate=0.3, max_depth=10, n_estimators=150, num_class=3, objective=multi:softmax;, score=0.817 total time=   1.0s\n",
      "[CV 1/5; 12/12] START learning_rate=0.3, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 1/5; 12/12] END learning_rate=0.3, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.812 total time=   1.6s\n",
      "[CV 2/5; 12/12] START learning_rate=0.3, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 2/5; 12/12] END learning_rate=0.3, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.807 total time=   1.6s\n",
      "[CV 3/5; 12/12] START learning_rate=0.3, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 3/5; 12/12] END learning_rate=0.3, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.820 total time=   1.6s\n",
      "[CV 4/5; 12/12] START learning_rate=0.3, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 4/5; 12/12] END learning_rate=0.3, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.817 total time=   1.6s\n",
      "[CV 5/5; 12/12] START learning_rate=0.3, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax\n",
      "[CV 5/5; 12/12] END learning_rate=0.3, max_depth=10, n_estimators=250, num_class=3, objective=multi:softmax;, score=0.817 total time=   1.6s\n",
      "XG Boost GridSearchCV best score = 0.8151535159529875\n",
      "XG Boost GridSearchCV best parameters = {'learning_rate': 0.3, 'max_depth': 8, 'n_estimators': 250, 'num_class': 3, 'objective': 'multi:softmax'}\n",
      "XG Boost GridSearchCV deceased class f1-score = 0.7377006394362522\n",
      "XG Boost GridSearchCV accuracy score = 0.8276501111934766\n"
     ]
    }
   ],
   "source": [
    "# Takes about 1 - 2 minutes to run.\n",
    "\n",
    "# Decide number of k-fold splits\n",
    "k = 5\n",
    "# Create model with blank parameters\n",
    "xgb_model = xgb.XGBClassifier(random_state = 1)\n",
    "# Create space of possible parameters\n",
    "parameter_search_space = {\n",
    "    \"learning_rate\": [0.2, 0.3],\n",
    "    \"max_depth\": [6, 8, 10],\n",
    "    \"n_estimators\": [150, 250],\n",
    "    \"objective\": [\"multi:softmax\"],\n",
    "    \"num_class\": [3]\n",
    "}\n",
    "# Create grid search cross validation object\n",
    "grid_search_cv = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=parameter_search_space,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=k,\n",
    "    verbose=10\n",
    ")\n",
    "# Put data and labels in proper format\n",
    "data = train_data.iloc[:, :4].values\n",
    "labels = train_data.iloc[:, 4].values.reshape(-1, 1)\n",
    "# Fit grid search object\n",
    "grid_search_cv.fit(data, labels)\n",
    "# Print and save results.\n",
    "print(\"XG Boost GridSearchCV best score = \" + str(grid_search_cv.best_score_))\n",
    "print(\"XG Boost GridSearchCV best parameters = \" + str(grid_search_cv.best_params_))\n",
    "predictions = grid_search_cv.predict(data)\n",
    "_, _, fscore, _ = precision_recall_fscore_support(predictions, labels)\n",
    "print(\"XG Boost GridSearchCV deceased class f1-score = \" + str(fscore[0]))\n",
    "accuracy = accuracy_score(predictions, labels)\n",
    "print(\"XG Boost GridSearchCV accuracy score = \" + str(accuracy))\n",
    "pd.DataFrame(grid_search_cv.cv_results_).to_csv(\"xgboost_results.csv\")\n",
    "xgb_model = grid_search_cv.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes about 3-4 minutes to run.\n",
    "\n",
    "# Decide number of k-fold splits\n",
    "k = 5\n",
    "# Create model with blank parameters\n",
    "rf_model = RandomForestClassifier(random_state = 44)\n",
    "# Create space of possible parameters\n",
    "parameter_search_space = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'criterion': [\"gini\", \"entropy\"],\n",
    "    'max_features': [1, 2, 3, 4]\n",
    "  }\n",
    "# Create grid search cross validation object\n",
    "grid_search_cv = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=parameter_search_space,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=k,\n",
    "    verbose=10\n",
    ")\n",
    "# Put data and labels in proper format\n",
    "data = train_data.iloc[:, :4].values\n",
    "labels = train_data.iloc[:, 4].values.ravel()\n",
    "# Fit grid search object\n",
    "grid_search_cv.fit(data, labels)\n",
    "# Print and save results.\n",
    "print(\"Random Forest GridSearchCV best score = \" + str(grid_search_cv.best_score_))\n",
    "print(\"Random Forest GridSearchCV best parameters = \" + str(grid_search_cv.best_params_))\n",
    "predictions = grid_search_cv.predict(data)\n",
    "_, _, fscore, _ = precision_recall_fscore_support(predictions, labels)\n",
    "print(\"Random Forest GridSearchCV deceased class f1-score = \" + str(fscore[0]))\n",
    "accuracy = accuracy_score(predictions, labels)\n",
    "print(\"Random Forest GridSearchCV accuracy score = \" + str(accuracy))\n",
    "pd.DataFrame(grid_search_cv.cv_results_).to_csv(\"rf_results.csv\")\n",
    "rf_model = grid_search_cv.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes about 12 minutes to run.\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# x_train = train_data[['age', 'country', 'chronic_disease_binary', 'Case_Fatality_Ratio']]\n",
    "# y_train = train_data[['outcome_group']]\n",
    "mlp_gs = MLPClassifier()\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(10,30,10),(20,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "data = train_data.iloc[:, :4].values\n",
    "labels = train_data.iloc[:, 4].values.reshape(-1, 1)\n",
    "grid_search_cv = GridSearchCV(\n",
    "    estimator=mlp_gs,\n",
    "    param_grid=parameter_space,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=5,\n",
    "    verbose=10\n",
    ")\n",
    "grid_search_cv.fit(data,labels.ravel())\n",
    "\n",
    "print(\"MLP Classifier GridSearchCV best score = \" + str(grid_search_cv.best_score_))\n",
    "print(\"MLP Classifier GridSearchCV best parameters = \" + str(grid_search_cv.best_params_))\n",
    "predictions = grid_search_cv.predict(data)\n",
    "_, _, fscore, _ = precision_recall_fscore_support(predictions, labels)\n",
    "print(\"MLP Classifier GridSearchCV deceased class f1-score = \" + str(fscore[0]))\n",
    "accuracy = accuracy_score(predictions, labels)\n",
    "print(\"MLP Classifier GridSearchCV accuracy score = \" + str(accuracy))\n",
    "pd.DataFrame(grid_search_cv.cv_results_).to_csv(\"MLP Classifier_results.csv\")\n",
    "mlp_gs = grid_search_cv.best_estimator_\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Hyper Parameter Results As Text Files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "criterion=gini,n_estimators=50,max_features=1 -> mean_macro_f1_score=0.8150136925404992,deceased_f1score=0.7294147046682057,accuracy=0.8198258665714221\n",
      "\n",
      "criterion=gini,n_estimators=152,max_features=2 -> mean_macro_f1_score=0.8156660073405828,deceased_f1score=0.7304317157789292,accuracy=0.8204024219531991\n",
      "\n",
      "criterion=gini,n_estimators=153,max_features=3 -> mean_macro_f1_score=0.8155120830777107,deceased_f1score=0.7303376452170739,accuracy=0.8202377112264131\n",
      "\n",
      "criterion=gini,n_estimators=153,max_features=4 -> mean_macro_f1_score=0.8151214020911279,deceased_f1score=0.7302632352557803,accuracy=0.8198258750511753\n",
      "\n",
      "criterion=gini,n_estimators=100,max_features=1 -> mean_macro_f1_score=0.8153279628980364,deceased_f1score=0.7294036085533686,accuracy=0.8201141866610768\n",
      "\n",
      "criterion=gini,n_estimators=153,max_features=2 -> mean_macro_f1_score=0.8156672860312512,deceased_f1score=0.7303850836168241,accuracy=0.8204024304329524\n",
      "\n",
      "criterion=gini,n_estimators=153,max_features=3 -> mean_macro_f1_score=0.8155120830777107,deceased_f1score=0.7303376452170739,accuracy=0.8202377112264131\n",
      "\n",
      "criterion=gini,n_estimators=153,max_features=4 -> mean_macro_f1_score=0.8151214020911279,deceased_f1score=0.7302632352557803,accuracy=0.8198258750511753\n",
      "\n",
      "criterion=gini,n_estimators=150,max_features=1 -> mean_macro_f1_score=0.8157360313289006,deceased_f1score=0.7303827236980176,accuracy=0.8204847942760987\n",
      "\n",
      "criterion=gini,n_estimators=153,max_features=2 -> mean_macro_f1_score=0.8156672860312512,deceased_f1score=0.7303850836168241,accuracy=0.8204024304329524\n",
      "\n",
      "criterion=gini,n_estimators=153,max_features=3 -> mean_macro_f1_score=0.8155120830777107,deceased_f1score=0.7303376452170739,accuracy=0.8202377112264131\n",
      "\n",
      "criterion=gini,n_estimators=153,max_features=4 -> mean_macro_f1_score=0.8151214020911279,deceased_f1score=0.7302632352557803,accuracy=0.8198258750511753\n",
      "\n",
      "criterion=gini,n_estimators=200,max_features=1 -> mean_macro_f1_score=0.8156116631844395,deceased_f1score=0.7304293855432302,accuracy=0.8203612273119962\n",
      "\n",
      "criterion=gini,n_estimators=153,max_features=2 -> mean_macro_f1_score=0.8156672860312512,deceased_f1score=0.7303850836168241,accuracy=0.8204024304329524\n",
      "\n",
      "criterion=gini,n_estimators=153,max_features=3 -> mean_macro_f1_score=0.8155120830777107,deceased_f1score=0.7303376452170739,accuracy=0.8202377112264131\n",
      "\n",
      "criterion=gini,n_estimators=153,max_features=4 -> mean_macro_f1_score=0.8151214020911279,deceased_f1score=0.7302632352557803,accuracy=0.8198258750511753\n",
      "\n",
      "criterion=entropy,n_estimators=50,max_features=1 -> mean_macro_f1_score=0.8149849710468693,deceased_f1score=0.7295087333823995,accuracy=0.8197846888897257\n",
      "\n",
      "criterion=entropy,n_estimators=155,max_features=2 -> mean_macro_f1_score=0.8155571225615914,deceased_f1score=0.7302875075520214,accuracy=0.8202788804283564\n",
      "\n",
      "criterion=entropy,n_estimators=156,max_features=3 -> mean_macro_f1_score=0.8155235001643486,deceased_f1score=0.730515640911569,accuracy=0.8202377027466599\n",
      "\n",
      "criterion=entropy,n_estimators=155,max_features=4 -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "criterion=entropy,n_estimators=100,max_features=1 -> mean_macro_f1_score=0.8152111212253386,deceased_f1score=0.7293513268318219,accuracy=0.8199906281767275\n",
      "\n",
      "criterion=entropy,n_estimators=156,max_features=2 -> mean_macro_f1_score=0.8155571225615914,deceased_f1score=0.7302875075520214,accuracy=0.8202788804283564\n",
      "\n",
      "criterion=entropy,n_estimators=156,max_features=3 -> mean_macro_f1_score=0.8155235001643486,deceased_f1score=0.730515640911569,accuracy=0.8202377027466599\n",
      "\n",
      "criterion=entropy,n_estimators=155,max_features=4 -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "criterion=entropy,n_estimators=150,max_features=1 -> mean_macro_f1_score=0.8158196781109407,deceased_f1score=0.7304781573884941,accuracy=0.8205671411597384\n",
      "\n",
      "criterion=entropy,n_estimators=156,max_features=2 -> mean_macro_f1_score=0.8155571225615914,deceased_f1score=0.7302875075520214,accuracy=0.8202788804283564\n",
      "\n",
      "criterion=entropy,n_estimators=156,max_features=3 -> mean_macro_f1_score=0.8155235001643486,deceased_f1score=0.730515640911569,accuracy=0.8202377027466599\n",
      "\n",
      "criterion=entropy,n_estimators=155,max_features=4 -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "criterion=entropy,n_estimators=200,max_features=1 -> mean_macro_f1_score=0.8157469714473884,deceased_f1score=0.7305612455591366,accuracy=0.8204847688368389\n",
      "\n",
      "criterion=entropy,n_estimators=156,max_features=2 -> mean_macro_f1_score=0.8155571225615914,deceased_f1score=0.7302875075520214,accuracy=0.8202788804283564\n",
      "\n",
      "criterion=entropy,n_estimators=156,max_features=3 -> mean_macro_f1_score=0.8155235001643486,deceased_f1score=0.730515640911569,accuracy=0.8202377027466599\n",
      "\n",
      "criterion=entropy,n_estimators=155,max_features=4 -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n"
     ]
    }
   ],
   "source": [
    "# UNCOMMENT TO REPRODUCE randomforest_tuning.txt FILE\n",
    "\"\"\"\n",
    "from sklearn.model_selection import KFold\n",
    "data = train_data.iloc[:, :4].values\n",
    "labels = train_data.iloc[:, 4].values.ravel()\n",
    "rf_model = RandomForestClassifier(random_state = 44)\n",
    "kf = KFold(n_splits=k)\n",
    "n_estimators = [50, 100, 150, 200]\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "max_features = [1, 2, 3, 4]\n",
    "text_file = open(\"randomforest_tuning.txt\", \"w\")\n",
    "for c in criterion:\n",
    "    for n in n_estimators:\n",
    "        for m in max_features:\n",
    "            rf_model.set_params(criterion = c, n_estimators = n, max_features = m)\n",
    "            deceased_f1score = 0\n",
    "            mean_macro_f1score = 0\n",
    "            accuracy = 0\n",
    "            for i, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "                train_fold_data = np.take(data, train_index, 0)\n",
    "                train_fold_labels = np.take(labels, train_index, 0)\n",
    "                test_fold_data = np.take(data, test_index, 0)\n",
    "                test_fold_labels = np.take(labels, test_index, 0)\n",
    "                rf_model.fit(train_fold_data, train_fold_labels)\n",
    "                predictions = rf_model.predict(test_fold_data)\n",
    "                _, _, fscore, _ = precision_recall_fscore_support(predictions, test_fold_labels)\n",
    "                deceased_f1score += fscore[0]\n",
    "                mean_macro_f1score += ( fscore[0] + fscore[1] + fscore[2] ) / 3\n",
    "                accuracy += accuracy_score(predictions, test_fold_labels)\n",
    "            deceased_f1score = deceased_f1score / k\n",
    "            mean_macro_f1score = mean_macro_f1score / k\n",
    "            accuracy = accuracy / k\n",
    "            output = \"\\ncriterion=\"+str(c)+\",n_estimators=\"+str(n)+\",max_features=\"+str(m)+\" -> mean_macro_f1_score=\"+str(mean_macro_f1score)+\",deceased_f1score=\"+str(deceased_f1score)+\",accuracy=\"+str(accuracy)\n",
    "            print(output)\n",
    "            n = text_file.write(output)\n",
    "text_file.close()\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning_rate=0.2,n_estimators=150,max_depth=6 -> mean_macro_f1_score=0.8135220243145304,deceased_f1score=0.7261081386042167,accuracy=0.8186316714034568\n",
      "\n",
      "learning_rate=0.2,n_estimators=153,max_depth=8 -> mean_macro_f1_score=0.8137886347202773,deceased_f1score=0.7270035925773769,accuracy=0.8187139758883305\n",
      "\n",
      "learning_rate=0.2,n_estimators=153,max_depth=10 -> mean_macro_f1_score=0.815123462767669,deceased_f1score=0.7291226916112434,accuracy=0.8199905518589485\n",
      "\n",
      "learning_rate=0.2,n_estimators=250,max_depth=6 -> mean_macro_f1_score=0.8139689928476109,deceased_f1score=0.7272859564100248,accuracy=0.8188374834941603\n",
      "\n",
      "learning_rate=0.2,n_estimators=153,max_depth=8 -> mean_macro_f1_score=0.8137886347202773,deceased_f1score=0.7270035925773769,accuracy=0.8187139758883305\n",
      "\n",
      "learning_rate=0.2,n_estimators=153,max_depth=10 -> mean_macro_f1_score=0.815123462767669,deceased_f1score=0.7291226916112434,accuracy=0.8199905518589485\n",
      "\n",
      "learning_rate=0.3,n_estimators=150,max_depth=6 -> mean_macro_f1_score=0.8138302963286751,deceased_f1score=0.7268853761321561,accuracy=0.8187963397314768\n",
      "\n",
      "learning_rate=0.3,n_estimators=153,max_depth=8 -> mean_macro_f1_score=0.814848377791116,deceased_f1score=0.7290411050019652,accuracy=0.81961992728442\n",
      "\n",
      "learning_rate=0.3,n_estimators=150,max_depth=10 -> mean_macro_f1_score=0.8154883159670125,deceased_f1score=0.7297088463590834,accuracy=0.8203612188322431\n",
      "\n",
      "learning_rate=0.3,n_estimators=250,max_depth=6 -> mean_macro_f1_score=0.8145949767819042,deceased_f1score=0.7288336501109098,accuracy=0.8193315987150122\n",
      "\n",
      "learning_rate=0.3,n_estimators=153,max_depth=8 -> mean_macro_f1_score=0.814848377791116,deceased_f1score=0.7290411050019652,accuracy=0.81961992728442\n",
      "\n",
      "learning_rate=0.3,n_estimators=150,max_depth=10 -> mean_macro_f1_score=0.8154883159670125,deceased_f1score=0.7297088463590834,accuracy=0.8203612188322431\n"
     ]
    }
   ],
   "source": [
    "# UNCOMMENT TO REPRODUCE xgboost_tuning.txt FILE\n",
    "\"\"\"\n",
    "from sklearn.model_selection import KFold\n",
    "xgb_model_clone = xgb.XGBClassifier(random_state = 1)\n",
    "kf = KFold(n_splits=k)\n",
    "learning_rate = [0.2, 0.3]\n",
    "max_depth = [6, 8, 10]\n",
    "n_estimators = [150, 250]\n",
    "objective = \"multi:softmax\"\n",
    "num_class = 3\n",
    "xgb_model_clone.set_params(objective = \"multi:softmax\", num_class = 3)\n",
    "text_file = open(\"xgboost_tuning.txt\", \"w\")\n",
    "for l in learning_rate:\n",
    "    for n in n_estimators:\n",
    "        for d in max_depth:\n",
    "            xgb_model_clone.set_params(learning_rate = l, n_estimators = n, max_depth=d)\n",
    "            deceased_f1score = 0\n",
    "            mean_macro_f1score = 0\n",
    "            accuracy = 0\n",
    "            for i, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "                train_fold_data = np.take(data, train_index, 0)\n",
    "                train_fold_labels = np.take(labels, train_index, 0)\n",
    "                test_fold_data = np.take(data, test_index, 0)\n",
    "                test_fold_labels = np.take(labels, test_index, 0)\n",
    "                xgb_model_clone.fit(train_fold_data, train_fold_labels)\n",
    "                predictions = xgb_model_clone.predict(test_fold_data)\n",
    "                _, _, fscore, _ = precision_recall_fscore_support(predictions, test_fold_labels)\n",
    "                deceased_f1score += fscore[0]\n",
    "                mean_macro_f1score += ( fscore[0] + fscore[1] + fscore[2] ) / 3\n",
    "                accuracy += accuracy_score(predictions, test_fold_labels)\n",
    "            deceased_f1score = deceased_f1score / k\n",
    "            mean_macro_f1score = mean_macro_f1score / k\n",
    "            accuracy = accuracy / k\n",
    "            output = \"\\nlearning_rate=\"+str(l)+\",n_estimators=\"+str(n)+\",max_depth=\"+str(d)+\" -> mean_macro_f1_score=\"+str(mean_macro_f1score)+\",deceased_f1score=\"+str(deceased_f1score)+\",accuracy=\"+str(accuracy)\n",
    "            print(output)\n",
    "            n = text_file.write(output)\n",
    "text_file.close()\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hidden_layer_sizes=(10, 30, 10),activation=tanh,solver=sgd,alpha=0.0001,learning_rate=constant -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(10, 30, 10),activation=tanh,solver=sgd,alpha=0.0001,learning_rate=adaptive -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(10, 30, 10),activation=tanh,solver=sgd,alpha=0.05,learning_rate=constant -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(10, 30, 10),activation=tanh,solver=sgd,alpha=0.05,learning_rate=adaptive -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(10, 30, 10),activation=tanh,solver=adam,alpha=0.0001,learning_rate=constant -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(10, 30, 10),activation=tanh,solver=adam,alpha=0.0001,learning_rate=adaptive -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(10, 30, 10),activation=tanh,solver=adam,alpha=0.05,learning_rate=constant -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(10, 30, 10),activation=tanh,solver=adam,alpha=0.05,learning_rate=adaptive -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(10, 30, 10),activation=relu,solver=sgd,alpha=0.0001,learning_rate=constant -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(10, 30, 10),activation=relu,solver=sgd,alpha=0.0001,learning_rate=adaptive -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(10, 30, 10),activation=relu,solver=sgd,alpha=0.05,learning_rate=constant -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(10, 30, 10),activation=relu,solver=sgd,alpha=0.05,learning_rate=adaptive -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(10, 30, 10),activation=relu,solver=adam,alpha=0.0001,learning_rate=constant -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(10, 30, 10),activation=relu,solver=adam,alpha=0.0001,learning_rate=adaptive -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(10, 30, 10),activation=relu,solver=adam,alpha=0.05,learning_rate=constant -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(10, 30, 10),activation=relu,solver=adam,alpha=0.05,learning_rate=adaptive -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(20,),activation=tanh,solver=sgd,alpha=0.0001,learning_rate=constant -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(20,),activation=tanh,solver=sgd,alpha=0.0001,learning_rate=adaptive -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(20,),activation=tanh,solver=sgd,alpha=0.05,learning_rate=constant -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(20,),activation=tanh,solver=sgd,alpha=0.05,learning_rate=adaptive -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(20,),activation=tanh,solver=adam,alpha=0.0001,learning_rate=constant -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(20,),activation=tanh,solver=adam,alpha=0.0001,learning_rate=adaptive -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(20,),activation=tanh,solver=adam,alpha=0.05,learning_rate=constant -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(20,),activation=tanh,solver=adam,alpha=0.05,learning_rate=adaptive -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(20,),activation=relu,solver=sgd,alpha=0.0001,learning_rate=constant -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(20,),activation=relu,solver=sgd,alpha=0.0001,learning_rate=adaptive -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(20,),activation=relu,solver=sgd,alpha=0.05,learning_rate=constant -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(20,),activation=relu,solver=sgd,alpha=0.05,learning_rate=adaptive -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(20,),activation=relu,solver=adam,alpha=0.0001,learning_rate=constant -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(20,),activation=relu,solver=adam,alpha=0.0001,learning_rate=adaptive -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(20,),activation=relu,solver=adam,alpha=0.05,learning_rate=constant -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n",
      "\n",
      "hidden_layer_sizes=(20,),activation=relu,solver=adam,alpha=0.05,learning_rate=adaptive -> mean_macro_f1_score=0.8153298056812126,deceased_f1score=0.7303349953227495,accuracy=0.8200317804191644\n"
     ]
    }
   ],
   "source": [
    "# UNCOMMENT TO REPRODUCE mlps_tuning.txt FILE\n",
    "\"\"\"\n",
    "from sklearn.model_selection import KFold\n",
    "data = train_data.iloc[:, :4].values\n",
    "labels = train_data.iloc[:, 4].values.ravel()\n",
    "mlp_gs = MLPClassifier()\n",
    "hidden_layer_sizes = [(10,30,10),(20,)]\n",
    "activation =  ['tanh', 'relu']\n",
    "solver =  ['sgd', 'adam']\n",
    "alpha =  [0.0001, 0.05]\n",
    "learning_rate =  ['constant','adaptive']\n",
    "\n",
    "kf = KFold(n_splits=k)\n",
    "text_file = open(\"mlp_tuning.txt\", \"w\")\n",
    "for h in hidden_layer_sizes:\n",
    "    for act in activation:\n",
    "        for s in solver:\n",
    "            for a in alpha:\n",
    "                for l in learning_rate:\n",
    "                    mlp_gs.set_params(hidden_layer_sizes = h, activation = act, solver = s, alpha = a, learning_rate = l)\n",
    "                    deceased_f1score = 0\n",
    "                    mean_macro_f1score = 0\n",
    "                    accuracy = 0\n",
    "                    for i, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "                        train_fold_data = np.take(data, train_index, 0)\n",
    "                        train_fold_labels = np.take(labels, train_index, 0)\n",
    "                        test_fold_data = np.take(data, test_index, 0)\n",
    "                        test_fold_labels = np.take(labels, test_index, 0)\n",
    "                        rf_model.fit(train_fold_data, train_fold_labels)\n",
    "                        predictions = rf_model.predict(test_fold_data)\n",
    "                        _, _, fscore, _ = precision_recall_fscore_support(predictions, test_fold_labels)\n",
    "                        deceased_f1score += fscore[0]\n",
    "                        mean_macro_f1score += ( fscore[0] + fscore[1] + fscore[2] ) / 3\n",
    "                        accuracy += accuracy_score(predictions, test_fold_labels)\n",
    "                    deceased_f1score = deceased_f1score / k\n",
    "                    mean_macro_f1score = mean_macro_f1score / k\n",
    "                    accuracy = accuracy / k\n",
    "                    output = \"\\nhidden_layer_sizes=\"+str(h)+\",activation=\"+str(act)+\",solver=\"+str(s)+\",alpha=\"+str(a)+\",learning_rate=\"+str(l)+\" -> mean_macro_f1_score=\"+str(mean_macro_f1score)+\",deceased_f1score=\"+str(deceased_f1score)+\",accuracy=\"+str(accuracy)\n",
    "                    print(output)\n",
    "                    n = text_file.write(output)\n",
    "text_file.close()\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Check for overfitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for overfitting on XG Boost model by comparing results on train versus validation datasets.\n",
    "train_data_formatted = train_data.iloc[:, :4].values\n",
    "train_labels_truth = train_data.iloc[:, 4].values.reshape(-1, 1)\n",
    "train_labels_predicted = xgb_model.predict(train_data_formatted)\n",
    "train_data_score = f1_score(train_labels_predicted, train_labels_truth, average = \"macro\")\n",
    "\n",
    "validation_data_formatted = validation_data.iloc[:, :4].values\n",
    "validation_labels_truth = validation_data.iloc[:, 4].values.reshape(-1, 1)\n",
    "validation_labels_predicted = xgb_model.predict(validation_data_formatted)\n",
    "validation_data_score = f1_score(validation_labels_predicted, validation_labels_truth, average = \"macro\")\n",
    "\n",
    "print(\"Training Dataset F1-Score = \" + str(train_data_score))\n",
    "print(\"Validation Dataset F1-Score = \" + str(validation_data_score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for overfitting on Random Forest model by comparing results on train versus validation datasets.\n",
    "train_data_formatted = train_data.iloc[:, :4].values\n",
    "train_labels_truth = train_data.iloc[:, 4].values.ravel()\n",
    "train_labels_predicted = rf_model.predict(train_data_formatted)\n",
    "train_data_score = f1_score(train_labels_predicted, train_labels_truth, average = \"macro\")\n",
    "\n",
    "validation_data_formatted = validation_data.iloc[:, :4].values\n",
    "validation_labels_truth = validation_data.iloc[:, 4].values.ravel()\n",
    "validation_labels_predicted = rf_model.predict(validation_data_formatted)\n",
    "validation_data_score = f1_score(validation_labels_predicted, validation_labels_truth, average = \"macro\")\n",
    "\n",
    "print(\"Training Dataset F1-Score = \" + str(train_data_score))\n",
    "print(\"Validation Dataset F1-Score = \" + str(validation_data_score))\n",
    "\n",
    "accuracy = accuracy_score(validation_labels_predicted, validation_labels_truth)\n",
    "print(\"Validation dataset accuracy score = \" + str(accuracy)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_formatted = train_data.iloc[:, :4].values\n",
    "train_labels_truth = train_data.iloc[:, 4].values.reshape(-1, 1)\n",
    "train_labels_predicted = mlp_gs.predict(train_data_formatted)\n",
    "train_data_score = f1_score(train_labels_predicted, train_labels_truth, average = \"macro\")\n",
    "\n",
    "validation_data_formatted = validation_data.iloc[:, :4].values\n",
    "validation_labels_truth = validation_data.iloc[:, 4].values.reshape(-1, 1)\n",
    "validation_labels_predicted = mlp_gs.predict(validation_data_formatted)\n",
    "validation_data_score = f1_score(validation_labels_predicted, validation_labels_truth, average = \"macro\")\n",
    "\n",
    "print(\"Training Dataset F1-Score = \" + str(train_data_score))\n",
    "print(\"Validation Dataset F1-Score = \" + str(validation_data_score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.7 Prediction on test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# USING XG BOOST FOR NOW BUT WE CAN SUB THIS OUT FOR BEST PERFORMING MODEL LATER\n",
    "testing_data = test_data.iloc[:, :4].values\n",
    "predicted_labels = xgb_model.predict(testing_data)\n",
    "# CHANGE MODEL NAME TO BEST PERFORMING MODEL LATER\n",
    "model_name = \"xgboost\"\n",
    "result_data_frame = pd.DataFrame(testing_data, columns=[\"age\", \"country\", \"chronic_disease_binary\", \"Case_Fatality_Ratio\"])\n",
    "\n",
    "# This function is from the TA\n",
    "def create_submission_file(y_preds, file_name):\n",
    "    with open(file_name, 'w') as csvfile:\n",
    "        wr = csv.writer(csvfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow([\"Id\", \"Prediction\"])\n",
    "        for i, pred in enumerate(y_preds):\n",
    "            wr.writerow([str(i), str(pred)])\n",
    "create_submission_file(predicted_labels, \"submission_\"+model_name+\".csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on the final model selection, we can delete one of these\n",
    "\n",
    "import csv\n",
    "testing_data = test_data.iloc[:, :4].values\n",
    "predicted_labels = rf_model.predict(testing_data)\n",
    "\n",
    "model_name = \"random_forest\"\n",
    "result_data_frame = pd.DataFrame(testing_data, columns=[\"age\", \"country\", \"chronic_disease_binary\", \"Case_Fatality_Ratio\"])\n",
    "\n",
    "# This function is from the TA\n",
    "def create_submission_file(y_preds, file_name):\n",
    "    with open(file_name, 'w') as csvfile:\n",
    "        wr = csv.writer(csvfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow([\"Id\", \"Prediction\"])\n",
    "        for i, pred in enumerate(y_preds):\n",
    "            wr.writerow([str(i), str(pred)])\n",
    "create_submission_file(predicted_labels, \"submission_\"+model_name+\".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ebeb2db109b4e4030ca6b0eb886199afc2cd913864cd051a344632d0064a896"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
